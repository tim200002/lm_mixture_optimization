name: bayesian_first_try
description: "Bayesian optimization first try"
train_data: [["Books", "/media/ssd1/tim/dolma_v1.6_small/tokenized/book/train/2048-v1/0/manifest.jsonl"], ["C4", "/media/ssd1/tim/dolma_v1.6_small/tokenized/c4/train/2048-v1/0/manifest.jsonl"], ["CC", "/media/ssd1/tim/dolma_v1.6_small/tokenized/cc_en_head/train/2048-v1/0/manifest.jsonl"], ["Wiki", "/media/ssd1/tim/dolma_v1.6_small/tokenized/en_simple_wiki_v0/train/2048-v1/0/manifest.jsonl"], ["pes2o_v2", "/media/ssd1/tim/dolma_v1.6_small/tokenized/pes2o_v2/train/2048-v1/0/manifest.jsonl"], ["reddit", "/media/ssd1/tim/dolma_v1.6_small/tokenized/reddit-v5-dedupe-pii-nsfw-toxic/train/2048-v1/0/manifest.jsonl"], ["stack-v4", "/media/ssd1/tim/dolma_v1.6_small/tokenized/stack-v4-train/train/2048-v1/0/manifest.jsonl"]]
val_data: [["Books", "/media/ssd1/tim/dolma_v1.6_small/tokenized/book/val/2048-v1/0/shard-0000000.tar"], ["C4", "/media/ssd1/tim/dolma_v1.6_small/tokenized/c4/val/2048-v1/0/shard-0000000.tar"], ["CC", "/media/ssd1/tim/dolma_v1.6_small/tokenized/cc_en_head/val/2048-v1/0/shard-0000000.tar"], ["Wiki", "/media/ssd1/tim/dolma_v1.6_small/tokenized/en_simple_wiki_v0/val/2048-v1/0/shard-0000000.tar"], ["pes2o_v2", "/media/ssd1/tim/dolma_v1.6_small/tokenized/pes2o_v2/val/2048-v1/0/shard-0000000.tar"], ["reddit", "/media/ssd1/tim/dolma_v1.6_small/tokenized/reddit-v5-dedupe-pii-nsfw-toxic/val/2048-v1/0/shard-0000000.tar"], ["stack-v4", "/media/ssd1/tim/dolma_v1.6_small/tokenized/stack-v4-train/val/2048-v1/0/shard-0000000.tar"]]
val_weights: [0, 0, 1, 0, 0, 0, 0]
workspace: /root/code/mixture_optimization/logs
data_workspace: /media/ssd1/tim/data_workspace
delete_dataset_after_run: False
max_no_runs: 1

weight_selector:
  type: "bayesian"
  no_weights: 7
  prior_weights: [0.2, 6.47, 74.56, 0.14, 2.29, 2.91, 13.43]

open_lm_config: # Config for openLM model, i.e model size, hyperparameters, ...
  # setup
  complete_train_token_count: 500000000
  model: open_lm_25m.json
  seed: 100
  data_key: txt
  report_to: tensorboard
  workers: 2
  epochs: 5
  log_every_n_steps: 20

  # training parameters
  global_batch_size: 16
  grad_clip_norm: 1
  lr: 0.0003
  warmup: 100
  wd: 0.033
  beta2: 0.95
  lr_cooldown_end: 0.00003
  z_loss_coefficient: 0.0001
  accum_freq: 1
  model_norm: gain_only_lp_layer_norm
  
  # validation parameters
  val_frequency: 5
  global_val_batch_size: 8
  val_num_samples: 250000

  # turn on or off parameters
  precision: amp_bfloat16
  qk_norm: true
  grad_checkpointing: false
  fsdp: false
  fsdp_amp: false
  
data_mixing:
  chunk_size: 2048
  shard_size: 2048
  oversample_factor: 1.5
  no_workers: 8
  

      