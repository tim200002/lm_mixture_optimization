name: uniform_scale_up_154m_cc_books_stack_pes2o_reddit
description: "Simplex sampling to find good overview of space between Books, CC and stack-v4"

domain_names: ["Books", "CC", "stack-v4", "Pes2O", "Reddit"] # To guarantee the same order even after restoring (dicts are not order preserving in yaml dump)
train_data:
  Books: "/media/ssd1/tim/dolma_v1.6_small/tokenized/book/train/2048-v1/0/manifest.jsonl"
  CC: "/media/ssd1/tim/dolma_v1.6_small/tokenized/cc_en_head/train/2048-v1/0/manifest.jsonl"
  stack-v4: "/media/ssd1/tim/dolma_v1.6_small/tokenized/stack-v4-train/train/2048-v1/0/manifest.jsonl"
  Pes2O: "/media/ssd1/tim/dolma_v1.6_small/tokenized/pes2o_v2/train/2048-v1/0/manifest.jsonl"
  Reddit: "/media/ssd1/tim/dolma_v1.6_small/tokenized/reddit-v5-dedupe-pii-nsfw-toxic/train/2048-v1/0/manifest.jsonl"

val_data:
  Books: "/media/ssd1/tim/dolma_v1.6_small/tokenized/book/val/2048-v1/0/shard-0000000.tar"
  CC: "/media/ssd1/tim/dolma_v1.6_small/tokenized/cc_en_head/val/2048-v1/0/shard-0000000.tar"
  stack-v4: "/media/ssd1/tim/dolma_v1.6_small/tokenized/stack-v4-train/val/2048-v1/0/shard-0000000.tar"
  Pes2O: "/media/ssd1/tim/dolma_v1.6_small/tokenized/pes2o_v2/val/2048-v1/0/shard-0000000.tar"
  Reddit: "/media/ssd1/tim/dolma_v1.6_small/tokenized/reddit-v5-dedupe-pii-nsfw-toxic/val/2048-v1/0/shard-0000000.tar"


val_weights: [1, 1, 1, 1, 1]
workspace: /root/code/mixture_optimization/logs
data_workspace: /media/ssd1/tim/data_workspace
delete_dataset_after_run: True
dataset_tag: "dolma_v1_6_small"

experiment_manager_config:
  no_experiments: 1
  weight_selector_config:
    type: "deterministic"
    no_weights: 5
    no_initializations: 1
    maximize: false
    kwargs:
      weights: [0.2, 0.2, 0.2, 0.2, 0.2]

open_lm_config: # Config for openLM model, i.e model size, hyperparameters, ...
  # setup
  complete_train_token_count: 3080000000
  model: open_lm_154m_v2.json
  seed: 100
  data_key: txt
  report_to: tensorboard
  workers: 2
  epochs: 5
  log_every_n_steps: 20

  # training parameters
  global_batch_size: 14
  grad_clip_norm: 1
  lr: 0.0003
  warmup: 100
  wd: 0.033
  beta2: 0.95
  lr_cooldown_end: 0.00003
  z_loss_coefficient: 0.0001
  accum_freq: 1
  model_norm: gain_only_lp_layer_norm
  
  # validation parameters
  val_frequency: 1
  global_val_batch_size: 8
  val_num_samples: 250000

  # turn on or off parameters
  precision: amp_bfloat16
  qk_norm: true
  grad_checkpointing: false
  fsdp: false
  fsdp_amp: false
  
data_mixing_config:
  chunk_size: 2048
  shard_size: 2048
  oversample_factor: 1.2
  no_workers: 8
  seed: 100
  

      
