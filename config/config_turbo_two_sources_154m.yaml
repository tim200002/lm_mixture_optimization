name: turbo-two-sources-154m
description: "Turbo optimization only between two sources with new bounds"
train_data:
  Books: "/media/ssd1/tim/dolma_v1.6_small/tokenized/book/train/2048-v1/0/manifest.jsonl"
  CC: "/media/ssd1/tim/dolma_v1.6_small/tokenized/cc_en_head/train/2048-v1/0/manifest.jsonl"
 
val_data:
  Books: "/media/ssd1/tim/dolma_v1.6_small/tokenized/book/val/2048-v1/0/shard-0000000.tar"
  CC: "/media/ssd1/tim/dolma_v1.6_small/tokenized/cc_en_head/val/2048-v1/0/shard-0000000.tar"
val_weights: [1,1]

workspace: /root/code/mixture_optimization/logs
data_workspace: /media/ssd1/tim/data_workspace
delete_dataset_after_run: True
dataset_tag: "dolma_v1_6_small"

experiment_manager_config:
  no_experiments: 1
  weight_selector_config:
    type: "turbo"
    no_weights: 2
    no_initializations: 4
    maximize: false

open_lm_config: # Config for openLM model, i.e model size, hyperparameters, ...
  # setup
  complete_train_token_count: 3080000000
  model: open_lm_154m.json
  seed: 100
  data_key: txt
  report_to: tensorboard
  workers: 2
  epochs: 5
  log_every_n_steps: 20

  # training parameters
  global_batch_size: 16
  grad_clip_norm: 1
  lr: 0.0003
  warmup: 100
  wd: 0.033
  beta2: 0.95
  lr_cooldown_end: 0.00003
  z_loss_coefficient: 0.0001
  accum_freq: 1
  model_norm: gain_only_lp_layer_norm
  
  # validation parameters
  val_frequency: 5
  global_val_batch_size: 8
  val_num_samples: 250000

  # turn on or off parameters
  precision: amp_bfloat16
  qk_norm: true
  grad_checkpointing: false
  fsdp: false
  fsdp_amp: false
  
data_mixing_config:
  chunk_size: 2048
  shard_size: 2048
  oversample_factor: 1.5
  no_workers: 8
  

      