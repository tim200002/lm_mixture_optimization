2024-06-11,11:57:54 | INFO | Running with a single process. Device cuda:0.
2024-06-11,11:58:06 | INFO | Model (has 1439895552 parameters):
2024-06-11,11:58:06 | INFO | Transformer(
  (post_embed_norm): Identity()
  (tok_embeddings): Embedding(50432, 2048)
  (layers): ModuleList(
    (0-23): 24 x Block(
      (attention): CustomAttn(
        (in_proj): Linear(in_features=2048, out_features=6144, bias=False)
        (out_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (pos_embed): RotaryWithCast()
        (q_norm): Identity()
        (k_norm): Identity()
      )
      (feed_forward): SwiGLU(
        (w12): Linear(in_features=2048, out_features=11264, bias=False)
        (w3): Linear(in_features=5632, out_features=2048, bias=False)
      )
      (attention_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (output): Linear(in_features=2048, out_features=50432, bias=False)
)
2024-06-11,11:58:06 | INFO | Params:
2024-06-11,11:58:06 | INFO |   accum_freq: 1
2024-06-11,11:58:06 | INFO |   attn_activation: None
2024-06-11,11:58:06 | INFO |   attn_name: auto
2024-06-11,11:58:06 | INFO |   attn_seq_scalar: None
2024-06-11,11:58:06 | INFO |   attn_seq_scalar_alpha: None
2024-06-11,11:58:06 | INFO |   average: None
2024-06-11,11:58:06 | INFO |   average_coefficients: None
2024-06-11,11:58:06 | INFO |   averagers: None
2024-06-11,11:58:06 | INFO |   beta1: 0.9
2024-06-11,11:58:06 | INFO |   beta2: 0.95
2024-06-11,11:58:06 | INFO |   checkpoint_path: /root/code/mixture_optimization/logs/Test_14/runs/run_0/open_lm/run_0/checkpoints
2024-06-11,11:58:06 | INFO |   copy_codebase: False
2024-06-11,11:58:06 | INFO |   data_key: txt
2024-06-11,11:58:06 | INFO |   data_tolerate_error_p: 0.09
2024-06-11,11:58:06 | INFO |   data_tolerate_num_ckpts: 0
2024-06-11,11:58:06 | INFO |   dataset_manifest: ['/media/ssd1/tim/data_workspace/Test_15/run_0/manifest.jsonl']
2024-06-11,11:58:06 | INFO |   dataset_resampled: False
2024-06-11,11:58:06 | INFO |   dataset_type: auto
2024-06-11,11:58:06 | INFO |   ddp_static_graph: False
2024-06-11,11:58:06 | INFO |   debug: False
2024-06-11,11:58:06 | INFO |   delete_previous_checkpoint: False
2024-06-11,11:58:06 | INFO |   device: cuda:0
2024-06-11,11:58:06 | INFO |   disable_buffer: False
2024-06-11,11:58:06 | INFO |   dist_backend: nccl
2024-06-11,11:58:06 | INFO |   dist_url: env://
2024-06-11,11:58:06 | INFO |   distill_model: None
2024-06-11,11:58:06 | INFO |   distill_pretrained: None
2024-06-11,11:58:06 | INFO |   distributed: False
2024-06-11,11:58:06 | INFO |   epochs: 10
2024-06-11,11:58:06 | INFO |   epochs_cooldown: None
2024-06-11,11:58:06 | INFO |   eps: 1e-08
2024-06-11,11:58:06 | INFO |   experimental_meta_device: False
2024-06-11,11:58:06 | INFO |   ffn_type: swiglu
2024-06-11,11:58:06 | INFO |   force_distributed: False
2024-06-11,11:58:06 | INFO |   force_min_lr: 0.0
2024-06-11,11:58:06 | INFO |   fsdp: False
2024-06-11,11:58:06 | INFO |   fsdp_amp: False
2024-06-11,11:58:06 | INFO |   fsdp_backward_prefetch: False
2024-06-11,11:58:06 | INFO |   fsdp_checkpoint: False
2024-06-11,11:58:06 | INFO |   fsdp_cpu_offload: False
2024-06-11,11:58:06 | INFO |   fsdp_hybrid: False
2024-06-11,11:58:06 | INFO |   fsdp_hybrid_o2: False
2024-06-11,11:58:06 | INFO |   fsdp_limit_all_gathers: False
2024-06-11,11:58:06 | INFO |   fsdp_pure_bf16: False
2024-06-11,11:58:06 | INFO |   fsdp_use_orig_params: False
2024-06-11,11:58:06 | INFO |   global_batch_size: 10
2024-06-11,11:58:06 | INFO |   global_val_batch_size: None
2024-06-11,11:58:06 | INFO |   grad_checkpointing: False
2024-06-11,11:58:06 | INFO |   grad_clip_norm: 1.0
2024-06-11,11:58:06 | INFO |   hf_fsdp_block: None
2024-06-11,11:58:06 | INFO |   hf_model: None
2024-06-11,11:58:06 | INFO |   hf_seq_len: None
2024-06-11,11:58:06 | INFO |   ignore_parse_errors: False
2024-06-11,11:58:06 | INFO |   load_pretrained_state: False
2024-06-11,11:58:06 | INFO |   local_rank: 0
2024-06-11,11:58:06 | INFO |   log_avg_model_training_loss: 0
2024-06-11,11:58:06 | INFO |   log_every_n_steps: 50
2024-06-11,11:58:06 | INFO |   log_level: 20
2024-06-11,11:58:06 | INFO |   log_local: False
2024-06-11,11:58:06 | INFO |   log_logit_mean: False
2024-06-11,11:58:06 | INFO |   log_path: /root/code/mixture_optimization/logs/Test_14/runs/run_0/open_lm/run_0/out.log
2024-06-11,11:58:06 | INFO |   logs: /root/code/mixture_optimization/logs/Test_14/runs/run_0/open_lm
2024-06-11,11:58:06 | INFO |   lr: 0.0003
2024-06-11,11:58:06 | INFO |   lr_cooldown_end: 3e-05
2024-06-11,11:58:06 | INFO |   lr_cooldown_power: 1.0
2024-06-11,11:58:06 | INFO |   lr_scheduler: cosine
2024-06-11,11:58:06 | INFO |   model: open_lm_1b
2024-06-11,11:58:06 | INFO |   model_norm: default_layer_norm
2024-06-11,11:58:06 | INFO |   moe_capacity_factor: 1.25
2024-06-11,11:58:06 | INFO |   moe_expert_model_parallelism: False
2024-06-11,11:58:06 | INFO |   moe_freq: 0
2024-06-11,11:58:06 | INFO |   moe_loss_weight: 0.1
2024-06-11,11:58:06 | INFO |   moe_num_experts: None
2024-06-11,11:58:06 | INFO |   moe_top_k: 2
2024-06-11,11:58:06 | INFO |   moe_weight_parallelism: False
2024-06-11,11:58:06 | INFO |   multiple_data_passes: False
2024-06-11,11:58:06 | INFO |   name: run_0
2024-06-11,11:58:06 | INFO |   no_set_device_rank: False
2024-06-11,11:58:06 | INFO |   optimizer: adamw
2024-06-11,11:58:06 | INFO |   per_gpu_batch_size: 10
2024-06-11,11:58:06 | INFO |   positional_embedding_type: rotary
2024-06-11,11:58:06 | INFO |   precision: amp
2024-06-11,11:58:06 | INFO |   preset_world_size: None
2024-06-11,11:58:06 | INFO |   pretrained: None
2024-06-11,11:58:06 | INFO |   qk_norm: False
2024-06-11,11:58:06 | INFO |   rank: 0
2024-06-11,11:58:06 | INFO |   remote_sync: None
2024-06-11,11:58:06 | INFO |   remote_sync_frequency: 300
2024-06-11,11:58:06 | INFO |   remote_sync_protocol: s3
2024-06-11,11:58:06 | INFO |   report_to: tensorboard
2024-06-11,11:58:06 | INFO |   resume: None
2024-06-11,11:58:06 | INFO |   save_frequency: 1
2024-06-11,11:58:06 | INFO |   save_most_recent: False
2024-06-11,11:58:06 | INFO |   seed: 0
2024-06-11,11:58:06 | INFO |   seq_len: 2048
2024-06-11,11:58:06 | INFO |   skip_scheduler: False
2024-06-11,11:58:06 | INFO |   squash_mask_left: False
2024-06-11,11:58:06 | INFO |   target_mask_individual: None
2024-06-11,11:58:06 | INFO |   target_mask_left: None
2024-06-11,11:58:06 | INFO |   tensorboard: True
2024-06-11,11:58:06 | INFO |   tensorboard_path: /root/code/mixture_optimization/logs/Test_14/runs/run_0/open_lm/run_0/tensorboard
2024-06-11,11:58:06 | INFO |   torchcompile: False
2024-06-11,11:58:06 | INFO |   torchscript: False
2024-06-11,11:58:06 | INFO |   trace: False
2024-06-11,11:58:06 | INFO |   train_data: None
2024-06-11,11:58:06 | INFO |   train_data_mix_weights: None
2024-06-11,11:58:06 | INFO |   train_data_upsampling_factors: None
2024-06-11,11:58:06 | INFO |   train_num_samples: 39062
2024-06-11,11:58:06 | INFO |   use_bn_sync: False
2024-06-11,11:58:06 | INFO |   use_bnb_linear: None
2024-06-11,11:58:06 | INFO |   val_data: None
2024-06-11,11:58:06 | INFO |   val_data_key: None
2024-06-11,11:58:06 | INFO |   val_frequency: 1
2024-06-11,11:58:06 | INFO |   val_iter_ci: 10000
2024-06-11,11:58:06 | INFO |   val_max_pop_ci: None
2024-06-11,11:58:06 | INFO |   val_num_samples: None
2024-06-11,11:58:06 | INFO |   val_seq_ci: False
2024-06-11,11:58:06 | INFO |   val_tok_ci: False
2024-06-11,11:58:06 | INFO |   vocab_size: 50432
2024-06-11,11:58:06 | INFO |   wandb: False
2024-06-11,11:58:06 | INFO |   wandb_notes: 
2024-06-11,11:58:06 | INFO |   wandb_project_name: open-lm
2024-06-11,11:58:06 | INFO |   warmup: 200
2024-06-11,11:58:06 | INFO |   wd: 0.1
2024-06-11,11:58:06 | INFO |   workers: 2
2024-06-11,11:58:06 | INFO |   world_size: 1
2024-06-11,11:58:06 | INFO |   z_loss_coefficient: 0.0
2024-06-11,11:58:06 | INFO | Precounting number of steps / tokens seen per checkpoint:
2024-06-11,11:58:06 | WARNING | Source defined by /media/ssd1/tim/data_workspace/Test_15/run_0/manifest.jsonl contains 1 shards that are smaller than 90% the size of the largest shard. These shards might cause deterioration in performance, with more samples being skipped than necessary. It is advised to make the shards more uniform.
2024-06-11,11:58:06 | INFO | ==> Checkpoint 1, steps 3276, tokens seen 67092480
2024-06-11,11:58:06 | WARNING | Source defined by /media/ssd1/tim/data_workspace/Test_15/run_0/manifest.jsonl contains 1 shards that are smaller than 90% the size of the largest shard. These shards might cause deterioration in performance, with more samples being skipped than necessary. It is advised to make the shards more uniform.
2024-06-11,11:58:06 | INFO | ==> Checkpoint 2, steps 6552, tokens seen 134184960
2024-06-11,11:58:06 | WARNING | Source defined by /media/ssd1/tim/data_workspace/Test_15/run_0/manifest.jsonl contains 1 shards that are smaller than 90% the size of the largest shard. These shards might cause deterioration in performance, with more samples being skipped than necessary. It is advised to make the shards more uniform.
2024-06-11,11:58:06 | INFO | ==> Checkpoint 3, steps 9828, tokens seen 201277440
2024-06-11,11:58:06 | WARNING | Source defined by /media/ssd1/tim/data_workspace/Test_15/run_0/manifest.jsonl contains 1 shards that are smaller than 90% the size of the largest shard. These shards might cause deterioration in performance, with more samples being skipped than necessary. It is advised to make the shards more uniform.
2024-06-11,11:58:06 | INFO | ==> Checkpoint 4, steps 13104, tokens seen 268369920
2024-06-11,11:58:06 | WARNING | Source defined by /media/ssd1/tim/data_workspace/Test_15/run_0/manifest.jsonl contains 1 shards that are smaller than 90% the size of the largest shard. These shards might cause deterioration in performance, with more samples being skipped than necessary. It is advised to make the shards more uniform.
2024-06-11,11:58:06 | INFO | ==> Checkpoint 5, steps 16380, tokens seen 335462400
2024-06-11,11:58:06 | WARNING | Source defined by /media/ssd1/tim/data_workspace/Test_15/run_0/manifest.jsonl contains 1 shards that are smaller than 90% the size of the largest shard. These shards might cause deterioration in performance, with more samples being skipped than necessary. It is advised to make the shards more uniform.
2024-06-11,11:58:06 | INFO | ==> Checkpoint 6, steps 19656, tokens seen 402554880
2024-06-11,11:58:06 | WARNING | Source defined by /media/ssd1/tim/data_workspace/Test_15/run_0/manifest.jsonl contains 1 shards that are smaller than 90% the size of the largest shard. These shards might cause deterioration in performance, with more samples being skipped than necessary. It is advised to make the shards more uniform.
2024-06-11,11:58:06 | INFO | ==> Checkpoint 7, steps 22932, tokens seen 469647360
2024-06-11,11:58:06 | WARNING | Source defined by /media/ssd1/tim/data_workspace/Test_15/run_0/manifest.jsonl contains 1 shards that are smaller than 90% the size of the largest shard. These shards might cause deterioration in performance, with more samples being skipped than necessary. It is advised to make the shards more uniform.
2024-06-11,11:58:06 | INFO | ==> Checkpoint 8, steps 26208, tokens seen 536739840
2024-06-11,11:58:06 | WARNING | Source defined by /media/ssd1/tim/data_workspace/Test_15/run_0/manifest.jsonl contains 1 shards that are smaller than 90% the size of the largest shard. These shards might cause deterioration in performance, with more samples being skipped than necessary. It is advised to make the shards more uniform.
2024-06-11,11:58:06 | INFO | ==> Checkpoint 9, steps 30872, tokens seen 632258560
2024-06-11,11:58:06 | WARNING | Source defined by /media/ssd1/tim/data_workspace/Test_15/run_0/manifest.jsonl contains 1 shards that are smaller than 90% the size of the largest shard. These shards might cause deterioration in performance, with more samples being skipped than necessary. It is advised to make the shards more uniform.
2024-06-11,11:58:06 | ERROR | Number of shards requested for a single epoch is more than the number of shards available. This means that the amount of data requested to train on is more than the dataloader can serve. This can either happen because there are not enough data to begin with, or data being skipped due to rounding errors. To alleviate the latter, consider making more uniform shards, and using less workers/GPUs. This will allow for better use of the dataset.
